{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of Population Predictive Checks (POP-PC) to Bayesian Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Population Predictive Checks [Ranganath & Blei 2019]\n",
    "- Examples of Dirichlet Process, Bayesian Linear Regression, LDA.\n",
    "- Not tried on Matrix Factorization (MF) and Gaussian Mixture Models (GMM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior Predictive Checks (PPC)\n",
    "\n",
    "\\begin{equation}\n",
    "    PPC(x^{obs}; g, d) = E[g(d(x^{rep}, \\theta), d(x^{obs}, \\theta)) | x^{obs}]\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "- __Issue__: Overfits due to \"double dipping\" the data.\n",
    "\n",
    "- __Solution__: Use Bootstrap/Cross-validation to obtain $x^{new}$ to model true distribution F.\n",
    "\n",
    "### Population Predictive Checks (POP-PC)\n",
    "\\begin{equation}\n",
    "    POPPC(x^{obs}, F; g, d) = E[g(d(x^{rep}, \\theta), d(x^{new}, \\theta)) | x^{obs}, F]\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MovieLens Dataset: ratings and metadata of movies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import util_pmf as util_pmf\n",
    "import util_gmm as util_gmm\n",
    "\n",
    "from collections import defaultdict\n",
    "from numpy.linalg import inv\n",
    "from pyro import poutine\n",
    "from pyro.infer.mcmc.api import MCMC\n",
    "from pyro.infer.mcmc import NUTS\n",
    "from pyro.infer.autoguide import AutoDelta\n",
    "from pyro.infer import SVI, TraceEnum_ELBO\n",
    "from pyro.infer import Predictive\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "assert pyro.__version__.startswith('0.5.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization (MF) using the MovieLens Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use 3 different MFs to predict ratings\n",
    "- Represent matrix with product of two matrices (User and Movies matrix)\n",
    "- Will only use movies that have more than 4 ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Ratings (All, Train, Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div><img src=\"images/allratings.png\", style=\"width: 250px; float: left;\"></div>\n",
    "\n",
    "<div><img src=\"images/trainratings.png\", style=\"width: 250px; float: left;\"></div>\n",
    "\n",
    "<div><img src=\"images/testratings.png\", style=\"width: 250px; float: left;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: Probabilistic Matrix Factorization (PMF)\n",
    "\n",
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement Probabilistic Matrix Factorization (PMF) model.\n",
    "\n",
    "Given rating matrix $R$,  user matrix $U$, movie matrix $V$, variance $\\sigma$, the likelihood of PMF is:\n",
    "\n",
    "\\begin{equation}\n",
    "    p(R_{ij} | U_i, V_j, \\mu, \\sigma^2) = N(R_{ij}| U_i, V_j, \\mu, \\sigma^2)\n",
    "\\end{equation}\n",
    "\n",
    "where the user and feature vectors are given Gaussian priors:\n",
    "\\begin{equation}\n",
    "    p(U|\\mu_U, \\sigma_U^2) = \\prod_{i=1}^{N_U}N(U_i|\\mu_U,\\sigma_U^2I) \\\\\n",
    "    p(V|\\mu_V, \\sigma_V^2) = \\prod_{j=1}^{N_V}N(V_j|\\mu_V,\\sigma_V^2I)\n",
    "\\end{equation}\n",
    "\n",
    "We set $\\mu_U=\\mu_V=0$ and $\\sigma_U=\\sigma_V=1$ here for simplicity.\n",
    "\n",
    "For $k$ (represents latent factors of user and feature vectors), we chose $k=10$ for all MF models in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data, k=10):\n",
    "    with pyro.plate('users', ratings_df.userId.nunique()):\n",
    "        u = pyro.sample('u', dist.MultivariateNormal(\n",
    "            torch.zeros(k), 1*torch.eye(k)))\n",
    "    with pyro.plate('movies', ratings_df.movieId.nunique()):\n",
    "        v = pyro.sample('v', dist.MultivariateNormal(\n",
    "            torch.zeros(k), 1*torch.eye(k)))\n",
    "    with poutine.mask(mask=torch.tensor(is_observed)):\n",
    "        pyro.sample(\"obs\", dist.Normal(torch.mm(u, v.T), 1), obs=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use NUTS to approximate posterior \n",
    "- Take average of samples as predicted ratings (Same for rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.set_rng_seed(2)\n",
    "kernel = NUTS(model, step_size=1e-4)\n",
    "mcmc = MCMC(kernel, num_samples=100, warmup_steps=100)\n",
    "mcmc.run(torch.tensor(zero_imputated_ratings))\n",
    "posterior_samples = mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criticism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of Distributions (PMF, Train, Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div><img src=\"images/pmf.png\", style=\"width: 270px; float: left;\"></div>\n",
    "\n",
    "<div><img src=\"images/trainratings.png\", style=\"width: 250px; float: left;\"></div>\n",
    "\n",
    "<div><img src=\"images/testratings.png\", style=\"width: 250px; float: left;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- vs train data, predicted distribution is shifted towards the left side, where mean of training data is located\n",
    "- vs test data, predictions are strongly underestimating movies with high rated movies\n",
    "\n",
    "Summary:\n",
    "- PMF model is overfitting by capturing mean of the train data strongly, preventing balanced predictions\n",
    "- Concentrated predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Bayesian Probabilistic Matrix Factorization (BPMF)\n",
    "\n",
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In BPMF, prior distribution is set on mean and variance of Normal distributions of each user and movie\n",
    "- Allows more flexibility in representing different preferences of users and movie trends, producing more balanced predictions.\n",
    "\n",
    "\\begin{equation}\n",
    "    p(R_{ij} | U_i, V_j, \\mu, \\sigma^2) = N(R_{ij}| U_i, V_j, \\mu, \\sigma^2)\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "    p(U_i|\\mu_{U_i}, \\sigma_{U_i}^2) = N(U_i|\\mu_{U_i},\\sigma_{U_i}^2I) \\\\\n",
    "    p(V_j|\\mu_{V_j}, \\sigma_{V_j}^2) =N(V_j|\\mu_{V_j},\\sigma_{V_j}^2I) \\\\\n",
    "    p(\\mu_{U_i}) = N(0,1) \\\\\n",
    "    p(\\sigma^2_{U_i}) = InvGam(1,1) \\\\\n",
    "    p(\\mu_{V_j}) = N(0,1) \\\\\n",
    "    p(\\sigma^2_{V_j}) = InvGam(1,1) \\\\\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data, k=10):\n",
    "    with pyro.plate('users', ratings_df.userId.nunique()):\n",
    "        with pyro.plate('user_k', k):\n",
    "            p_u_mu = pyro.sample('p_u_mu', dist.Normal(0, 1))\n",
    "            p_u_std = pyro.sample('p_u_std', dist.InverseGamma(1, 1))\n",
    "            u = pyro.sample('u', dist.Normal(p_u_mu, p_u_std))\n",
    "    with pyro.plate('movies', ratings_df.movieId.nunique()):\n",
    "        with pyro.plate('movie_k', k):\n",
    "            p_v_mu = pyro.sample('p_v_mu', dist.Normal(0, 1))\n",
    "            p_v_std = pyro.sample('p_v_std', dist.InverseGamma(1, 1))\n",
    "            v = pyro.sample('v', dist.Normal(p_v_mu, p_v_std))\n",
    "    with poutine.mask(mask=torch.tensor(is_observed)):\n",
    "        pyro.sample(\"obs\", dist.Normal(torch.mm(u.T, v), 1), obs=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.set_rng_seed(2)\n",
    "kernel = NUTS(model, step_size=1e-4)\n",
    "mcmc = MCMC(kernel, num_samples=100, warmup_steps=100)\n",
    "mcmc.run(torch.tensor(zero_imputated_ratings))\n",
    "posterior_samples = mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criticism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare BPMF and PMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div><img src=\"images/bpmf.png\", style=\"width: 250px; float: left;\"></div>\n",
    "\n",
    "<div><img src=\"images/pmf.png\", style=\"width: 250px; float: left;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Estimated ratings of BPMF are more spread out, having more weight on high rated movies\n",
    "- BPMF has difference in proportion of low/high ratings compared to train and test data\n",
    "- Assumed distribution may be cause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Poisson Matrix Factorization (PoiMF)\n",
    "\n",
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Poisson distribution is assumed to generate ratings in matrix\n",
    "- Assumed distributions/priors are Poisson and Gamma\n",
    "\n",
    "\\begin{equation}\n",
    "    p(R_{ij} | U_i, V_j, e_i, n_j) = Poisson(R_{ij}|U_i, V_j, e_i, n_j) \n",
    "\\end{equation}\n",
    "\n",
    "where \n",
    "\\begin{equation}\n",
    "    p(U_i| e_i) = Gamma(1, e_i) \\\\\n",
    "    p(V_j| n_j) = Gamma(1, n_j) \\\\\n",
    "    p(e_i) = Gamma(1,1) \\\\\n",
    "    p(n_j) = Gamma(1,1)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data, k=10):\n",
    "    with pyro.plate('users', ratings_df.userId.nunique()):\n",
    "        e_u = pyro.sample('e_u', dist.Gamma(1, 1))\n",
    "        with pyro.plate('user_k', k):\n",
    "            theta_u_k = pyro.sample('theta_u_k', dist.Gamma(1, e_u))\n",
    "    with pyro.plate('movies', ratings_df.movieId.nunique()):\n",
    "        n_i = pyro.sample('n_i', dist.Gamma(1, 1))\n",
    "        with pyro.plate('movie_k', k):\n",
    "            beta_i_k = pyro.sample('beta_i_k', dist.Gamma(1, n_i))\n",
    "    with poutine.mask(mask=torch.tensor(is_observed)):\n",
    "        pyro.sample(\"obs\", dist.Poisson(torch.mm(\n",
    "            theta_u_k.T, beta_i_k)), obs=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.set_rng_seed(2)\n",
    "kernel = NUTS(model, step_size=1e-4)\n",
    "mcmc = MCMC(kernel, num_samples=1, warmup_steps=1)\n",
    "mcmc.run(torch.tensor(zero_imputated_ratings))\n",
    "posterior_samples = mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criticism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of Estimated Ratings (PoiMF, BPMF, PMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div><img src=\"images/poimf.png\", style=\"width: 250px; float: left;\"></div>\n",
    "\n",
    "<div><img src=\"images/bpmf.png\", style=\"width: 250px; float: left;\"></div>\n",
    "\n",
    "<div><img src=\"images/pmf.png\", style=\"width: 250px; float: left;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PoiMF has more high rating estimates compared to BPMF and PMF (closer to mode of ratings of train data)\n",
    "- PoiMF does not consider lower ratings as much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criticism using POP-PC and PPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppc_mean, ppc_median, ppc_first_q, ppc_third_q, \\\n",
    "    poppc_mean, poppc_median, poppc_first_q, poppc_third_q \\\n",
    "    = util_pmf.model_assessment(train_ratings, test_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/POPPCPPC.png\", style=\"width: 350px; float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Findings:__\n",
    "- PPC overestimates fit of model\n",
    "- PoiMF had lowest mean discrepancy for k=10\n",
    "- Calculation of PPC and POP-PC is difficult for Bayesian models that take time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Model for MovieLens dataset\n",
    "\n",
    "__Overview:__ \n",
    "* Explore two different GMM models using different priors and inference methods.\n",
    "* Choose 5-10 cluster based on fact that top movie genres are: Comedy, Drama, Thriller etc, and release periods: 2010s, 2000s, 1990s, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_metadata_dir = os.path.join(\"data\", \"movies_metadata_processed.csv\")\n",
    "movies_metadata = pd.read_csv(movies_metadata_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: SVI and Normal Inverse-Gamma Prior\n",
    "\n",
    "### Model\n",
    "\n",
    "Given $N$ data points, likelihood is:\n",
    "\\begin{equation}\n",
    "    p(x_n | \\pi, \\mu, \\sigma^2) = \\sum_{k=1}^{K} \\pi_k N(x_n | \\mu_k, \\sigma_k^2)\n",
    "\\end{equation}\n",
    "\n",
    "Define priors $\\pi \\in [0,1]$ s.t. $\\sum_{k=1}^{K} \\pi_k = 1$, $\\mu_k \\in \\mathbb{R}^D$, $\\sigma_k^2 \\in \\mathbb{R}^D$:\n",
    "\\begin{equation}\n",
    "    \\pi \\sim Dir(\\pi | \\alpha \\mathbb{1}_K)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mu_k \\sim Normal(\\mu_k | 0, I)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sigma_k^2 \\sim InvGamma(\\sigma_k^2 | 1, 1)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, data_new = util_gmm.get_train_test_split(movies_metadata)\n",
    "K = 8\n",
    "d = len(data[0])\n",
    "N = len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data):\n",
    "    weights = pyro.sample('weights', dist.Dirichlet(torch.ones(K)))\n",
    "    with pyro.plate('component', K):\n",
    "        locs = pyro.sample('locs', dist.MultivariateNormal(\n",
    "            torch.zeros(d), torch.eye(d)))\n",
    "        scale = pyro.sample('scale', dist.InverseGamma(1, 1))\n",
    "    with pyro.plate('data', len(data)):\n",
    "        assignment = pyro.sample('assignment', dist.Categorical(weights))\n",
    "        scale_ = scale[assignment].repeat(d, d, 1).transpose(2, 0)\n",
    "        scale_ *= torch.eye(d).repeat(len(assignment), 1, 1)\n",
    "        pyro.sample('obs', dist.MultivariateNormal(\n",
    "            locs[assignment], scale_), obs=data)\n",
    "\n",
    "\n",
    "global_guide = AutoDelta(\n",
    "    poutine.block(model, expose=['weights', 'locs', 'scale']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Run SVI and compute MAP estimates of posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = pyro.optim.Adam({'lr': 0.1})\n",
    "elbo = TraceEnum_ELBO()\n",
    "svi = SVI(model, global_guide, optim, loss=elbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(seed):\n",
    "    pyro.set_rng_seed(seed)\n",
    "    pyro.clear_param_store()\n",
    "    loss = svi.loss(model, global_guide, data)\n",
    "    return loss\n",
    "\n",
    "\n",
    "loss, seed = min((initialize(seed), seed) for seed in range(100))\n",
    "initialize(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_norms = defaultdict(list)\n",
    "for name, value in pyro.get_param_store().named_parameters():\n",
    "    value.register_hook(\n",
    "        lambda g, name=name: gradient_norms[name].append(g.norm().item()))\n",
    "\n",
    "losses = []\n",
    "for i in range(500):\n",
    "    loss = svi.step(data)\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_estimates = global_guide(data)\n",
    "weights = map_estimates['weights']\n",
    "locs = map_estimates['locs']\n",
    "scale = map_estimates['scale']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criticism\n",
    "\n",
    "ELBO decreases in general but increases midway. Gradients of scale, locs, and weights are noisy.\n",
    "\n",
    "__Issues:__ \n",
    "* Priors collapses to local minimum during SVI - all clusters tend to converge to same point. \n",
    "* Not fully probabilistic - Inverse Gamma prior for variance does not capture cross-covariances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util_gmm.plot_svi_convergence(losses, gradient_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util_gmm.plot_gmm_results(data, locs, scale, K, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Alternative:__ \n",
    "* MCMC for full posterior over components. \n",
    "* For multivariate covariance matrix, conjugate prior is Inverse-Wishart distribution.\n",
    "* However, Inverse Wishart is not suitable prior. It is heavy-tailed; which are obstructions to ergodicity requirement for MCMC. \n",
    "\n",
    "__Solution:__ \n",
    "* LKJCorr prior for full probabilistic covariance. \n",
    "* LKFCorr distribution has lighter tails \n",
    "* Cholesky factorization for faster computation, $\\Sigma= L L^T$\n",
    "\n",
    "\n",
    "### Experiment 2: NUTS and Normal-LKJCorr Prior\n",
    "\n",
    "### Model\n",
    "\n",
    "Given same setup before, define covariance matrix as:\n",
    "\\begin{equation}\n",
    "    \\Sigma = diag(\\theta) \\Omega diag(\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "$\\theta$ scale vector, $\\Omega$ correlation matrix:\n",
    "\\begin{align}\n",
    "    \\theta &\\sim HalfCauchy(1) \\\\\n",
    "    \\Omega &\\sim LKJCorr(\\eta) \\\\\n",
    "\\end{align}\n",
    "\n",
    "LKJ correlation distribution:\n",
    "\\begin{equation}\n",
    "    LKJCorr(\\Sigma | \\eta) \\propto det(\\Sigma)^{\\eta -1}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data):\n",
    "    weights = pyro.sample('weights', dist.Dirichlet(torch.ones(K)))\n",
    "    theta = pyro.sample(\"theta\", dist.HalfCauchy(1*torch.ones(d)).to_event(1))\n",
    "    eta = torch.ones(1)\n",
    "    L_omega = pyro.sample(\"L_omega\", dist.LKJCorrCholesky(d, eta))\n",
    "    L_Omega = torch.mm(torch.diag(theta.sqrt()), L_omega)\n",
    "    with pyro.plate('components', K):\n",
    "        mu = pyro.sample('locs', dist.MultivariateNormal(\n",
    "            torch.zeros(d), 1*torch.eye(d)))\n",
    "    with pyro.plate('data', len(data)):\n",
    "        assignment = pyro.sample('assignment', dist.Categorical(weights))\n",
    "        pyro.sample(\"obs\", dist.MultivariateNormal(\n",
    "            mu[assignment], scale_tril=L_Omega), obs=data)\n",
    "\n",
    "\n",
    "global_guide = AutoDelta(\n",
    "    poutine.block(model, expose=['weights', 'locs', 'L_omega']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Run NUTS over 250 samples, draw trace plots of posterior samples and compute Bayes estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.set_rng_seed(2)\n",
    "kernel = NUTS(model, step_size=1e-4)\n",
    "mcmc = MCMC(kernel, num_samples=250, warmup_steps=50)\n",
    "mcmc.run(data)\n",
    "posterior_samples = mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma_samples = util_gmm.get_Sigma_samples(posterior_samples)\n",
    "cov = util_gmm.get_bayes_estimate_cov(Sigma_samples, K)\n",
    "mu = util_gmm.get_bayes_estimate_mu(posterior_samples)\n",
    "pi = util_gmm.get_bayes_estimate_pi(posterior_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "util_gmm.plot_mcmc_mu(posterior_samples, K, d)\n",
    "util_gmm.plot_mcmc_pi(posterior_samples, K, d)\n",
    "util_gmm.plot_mcmc_theta(posterior_samples, K, d)\n",
    "util_gmm.plot_mcmc_Sigma(Sigma_samples, K, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criticism\n",
    "\n",
    "__Results:__ \n",
    "* Components of $\\mu_k, \\Sigma_k$ seem to capture general distribution of data well.\n",
    "* Most of density in GMM is clustered within single component.\n",
    "* Looking at the assignments, there is no significant pattern (genres/release periods) within each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util_gmm.plot_gmm_results(data, mu, cov, K, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive = Predictive(model, posterior_samples)\n",
    "assignment = predictive.get_samples(data)[\"assignment\"][-1, :]\n",
    "util_gmm.plot_assignments(assignment, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPC\n",
    "\n",
    "Use realized discrepancy $d(x, \\theta)$, which is function of both data and hidden variables. Ex. log-likelihood for GMMs:\n",
    "\\begin{align}\n",
    "    d(x, \\theta) \n",
    "    &= \\sum_{i=1}^{n} \\log p(x_i | \\theta)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute replicated samples from posterior predictive\n",
    "data_rep = util_gmm.get_replicated_data(data, mu, cov, pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_obs = util_gmm.compute_log_likelihood(data, mu, cov, pi)\n",
    "d_rep = util_gmm.compute_log_likelihood(data_rep, mu, cov, pi)\n",
    "ppc = np.sqrt((d_obs - d_rep)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POP-PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_new = util_gmm.compute_log_likelihood(data_new, mu, cov, pi)\n",
    "d_rep = util_gmm.compute_log_likelihood(data_rep, mu, cov, pi)\n",
    "pop_pc = np.sqrt((d_new - d_rep)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util_gmm.plot_rep_obs_new_data(data, data_rep, data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing  K using PPC and POP-PC \n",
    "\n",
    "Data is collected from repeating experiments above and changing K to observe how PPCs/POP-PCs respond.\n",
    "\n",
    "__Findings:__ \n",
    "* POP-PCs are higher than PPCs which is expected.\n",
    "* As $K$ increases from 10 onwards, discrepancy of POP-PCs increases while PPCs do not increase as much; PPC/POP-PC scores are optimal between 6-10. \n",
    "* *Zig-Zag* pattern s.t. *even* clusters tend to have lower scores; *odd* clusters have higher scores, suggesting that optimal number of clusters is 6, 8, or 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util_gmm.plot_ppc_vs_poppc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
