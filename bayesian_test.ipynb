{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application of Population Predictive Checks to Nonmentioned Bayesian Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Matrix Factorization using the MovieLens Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle \n",
    "import util_pmf as util_pmf\n",
    "\n",
    "from pyro import poutine\n",
    "from numpy.linalg import inv\n",
    "from pyro.infer.mcmc.api import MCMC\n",
    "from pyro.infer.mcmc import NUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum number of ratings for each user is 20\n",
      "Minimum number of ratings for each movie is 1\n"
     ]
    }
   ],
   "source": [
    "# Run this when you haven't created a indexed_ratings.csv yet\n",
    "# Data: https://www.kaggle.com/rounakbanik/the-movies-dataset/download\n",
    "ratings_df = pd.read_csv('../data/the-movies-dataset/ratings_small.csv')\n",
    "\n",
    "# Check the minimum number of ratings a user has posted\n",
    "print('Minimum number of ratings for each user is ' + str(ratings_df.groupby('userId').size().min()))\n",
    "print('Minimum number of ratings for each movie is ' + str(ratings_df.groupby('movieId').size().min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only use movies that have more than 4 ratings, to have more training data available per movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum number of ratings for each movie is 5\n"
     ]
    }
   ],
   "source": [
    "# We will only use part of the data that has more than 4 reviews per movie\n",
    "moviecounts = ratings_df.movieId.value_counts()\n",
    "ratings_df = ratings_df[ratings_df.movieId.isin(moviecounts.index[moviecounts.gt(4)])].reset_index(drop=True)\n",
    "print('Minimum number of ratings for each movie is ' + str(ratings_df.groupby('movieId').size().min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this when you DON'T have a train_ratings.csv \n",
    "# Randomly split the data into train and test\n",
    "# Specifically, take 5 random ratings out of each user's rating list (therefore, there will be 5 times # of user ratings in the test set)\n",
    "# For each user id, take out 5 ratings oaut of the dataframe then append them into a new dataframe.\n",
    "# Then take the difference of the two data frames and the difference will be the train data\n",
    "\n",
    "def train_test_split_mf(ratings_df):\n",
    "    for userid in ratings_df.userId.unique() :\n",
    "        if userid == 1:\n",
    "            test_ratings = ratings_df[ratings_df['userId']==userid].sample(5, random_state=0)\n",
    "        else:\n",
    "            test_ratings = test_ratings.append(ratings_df[ratings_df['userId']==userid].sample(5, random_state=0))\n",
    "    train_ratings = pd.concat([ratings_df, test_ratings]).drop_duplicates(keep=False).reset_index(drop=True)\n",
    "    test_ratings = test_ratings.reset_index(drop=True)\n",
    "    print(\"Number of ratings in entire dataset is \"+ str(len(ratings_df)))\n",
    "    print(\"Number of ratings in train dataset is \"+  str(len(train_ratings)))\n",
    "    print(\"Number of ratings in test dataset is \"+ str(len(test_ratings)))\n",
    "    return train_ratings, test_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratings, test_ratings = train_test_split_mf(ratings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratings.to_csv('../data/train_ratings.csv')\n",
    "test_ratings.to_csv('../data/test_ratings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split randomly the data into train and test data. Specifically, we take 5 random ratings out of each user's rating list, creating 5 x (num of users) ratings in the test set. The remaining data becomes the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this when you have a train_ratings.csv \n",
    "train_ratings = pd.read_csv('../data/train_ratings.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the IDs of the users and movies have gaps in between, we reindex the IDs so that it will be easier to manipulate the user x movie ratings matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this when you DON'T have a indexed_train_ratings.csv \n",
    "# Reindexing of userId and movieId\n",
    "unique_userId = train_ratings.userId.unique()\n",
    "unique_movieId = train_ratings.movieId.unique()\n",
    "\n",
    "train_ratings['new_user_index'], train_ratings['new_movie_index'] = 0, 0\n",
    "\n",
    "for old_id, new_id in zip(unique_userId, range(len(unique_userId))):\n",
    "    train_ratings['new_user_index'].iloc[train_ratings[train_ratings['userId']==old_id].index.tolist()] = new_id\n",
    "\n",
    "for old_id, new_id in zip(unique_movieId, range(len(unique_movieId))):\n",
    "    train_ratings['new_movie_index'].iloc[train_ratings[train_ratings['movieId']==old_id].index.tolist()] = new_id\n",
    "\n",
    "train_ratings.to_csv('../data/indexed_train_ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this when you have a indexed_train_ratings.csv \n",
    "train_ratings = pd.read_csv('../data/indexed_train_ratings.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of All Ratings (with more than 4 reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Distribution of All Ratings (with more than 4 reviews)\n",
    "ratings_df['rating'].value_counts().reindex([0.5,1.0,1.5,2.0,2.5,3.0,3.5,4.0,4.5,5.0]).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of Train Data Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Train Data Ratings\n",
    "train_ratings['rating'].value_counts().reindex([0.5,1.0,1.5,2.0,2.5,3.0,3.5,4.0,4.5,5.0]).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of Train Data Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Test Data Ratings\n",
    "test_ratings['rating'].value_counts().reindex([0.5,1.0,1.5,2.0,2.5,3.0,3.5,4.0,4.5,5.0]).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement the Probabilistic Matrix Factorization (PMF) model, and train the model using the train dataset. In our case, PMF 's objective is to fill in the missing values of the movie matrix $M$, by maximizing the following MAP objective function.\n",
    "\n",
    "$L = - \\Sigma_{(i,j)\\in \\Omega}\\frac{1}{2\\sigma^2}(M_{ij}-u_i^Tv_j)^2-\\Sigma_{i=1}^{N_u}\\frac{\\lambda}{2}||u_i||^2-\\Sigma_{j=1}^{N_v}\\frac{\\lambda}{2}||v_j||^2$,\n",
    "\n",
    "where $u_i$ is the $i$th user vector, $v_j$ the $j$th movie vector, $N_u$ the number of users, $N_v$ the number of movies, $\\Omega$ the set of $(i,j)$s that have a rating in the matrix $M$, $\\sigma^2$ the variance of $M_{ij} \\sim N(u_i^Tv_j, \\sigma^2)$. Therefore, each rating in $M$ are assumed to be normally distributed. Also, note that the each user vector $u_i$ and movie vector $v_j$ are initialized from the distribution $N(0, \\lambda^{-1}I)$, where $u$ is a $i \\times k$ matrix and $v$ is a $j \\times k$ matrix.\n",
    "\n",
    "We have implemented the model below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation of train data using the mean ratings of each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this when you DON'T have a mean_imputated_ratings.npy\n",
    "\n",
    "imputated_ratings = np.empty((ratings_df.userId.nunique(),ratings_df.movieId.nunique()))\n",
    "\n",
    "for user in range(ratings_df.userId.nunique()):\n",
    "    imputated_ratings[user] = train_ratings[train_ratings.new_user_index == user]['rating'].mean()\n",
    "    for column in train_ratings[train_ratings.new_user_index == user]['new_movie_index']:\n",
    "        imputated_ratings[user, column] = train_ratings[(train_ratings.new_user_index == user)&(train_ratings.new_movie_index == column)]['rating']   \n",
    "\n",
    "np.save('../data/mean_imputated_ratings.npy', imputated_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this when you have a imputated_ratings.npy\n",
    "imputated_ratings = np.load('../data/mean_imputated_ratings.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation of train data using zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this when you DON'T have a zero_imputated_ratings.npy\n",
    "\n",
    "imputated_ratings = np.empty((ratings_df.userId.nunique(),ratings_df.movieId.nunique()))\n",
    "\n",
    "for user in range(ratings_df.userId.nunique()):\n",
    "    imputated_ratings[user] = 0\n",
    "    for column in train_ratings[train_ratings.new_user_index == user]['new_movie_index']:\n",
    "        imputated_ratings[user, column] = train_ratings[(train_ratings.new_user_index == user)&(train_ratings.new_movie_index == column)]['rating']  \n",
    "\n",
    "np.save('../data/zero_imputated_ratings.npy', imputated_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this when you have a zero_imputated_ratings.npy\n",
    "imputated_ratings = np.load('../data/zero_imputated_ratings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of Probabilistic Matrix Factorization using Pyro\n",
    "# (To use Pyro, need data that has missing values imputated with the mean rating of each user)\n",
    "def model(data, k=10):\n",
    "    with pyro.plate('users', ratings_df.userId.nunique()):\n",
    "        u = pyro.sample('u', dist.MultivariateNormal(torch.zeros(k), 1*torch.eye(k)))\n",
    "    with pyro.plate('movies', ratings_df.movieId.nunique()):\n",
    "        v = pyro.sample('v', dist.MultivariateNormal(torch.zeros(k), 1*torch.eye(k)))\n",
    "    r = pyro.sample(\"obs\", dist.Normal(torch.mm(u,v.T), 1), obs=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.set_rng_seed(2)\n",
    "kernel = NUTS(model, step_size=1e-4) \n",
    "mcmc = MCMC(kernel, num_samples=5, warmup_steps=25)\n",
    "mcmc.run(torch.tensor(imputated_ratings))\n",
    "posterior_samples = mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(posterior_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pmf_dict.pickle', 'wb') as handle:\n",
    "    pickle.dump(posterior_samples, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(torch.mm(posterior_samples['u'][i,:,:].T, posterior_samples['v'][i,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor =  torch.mm(posterior_samples['u'][0,:,:], posterior_samples['v'][0,:,:].T)\n",
    "for i in range(1,5):\n",
    "    tensor += torch.mm(posterior_samples['u'][i,:,:], posterior_samples['v'][i,:,:].T)\n",
    "tensor = tensor/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_estimate_ratings = (tensor* 2).round()/ 2\n",
    "rounded_estimate_ratings[rounded_estimate_ratings>5.0] = 5.0\n",
    "rounded_estimate_ratings[rounded_estimate_ratings<0.5] = 0.5\n",
    "print(rounded_estimate_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_estimate_ratings_unique = rounded_estimate_ratings.unique(sorted=True)\n",
    "rounded_estimate_ratings_counts = torch.stack([(rounded_estimate_ratings==rounded_estimate).sum() for  rounded_estimate in rounded_estimate_ratings_unique])\n",
    "rounded_estimate_ratings_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(rounded_estimate_ratings_unique, rounded_estimate_ratings_counts, width=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the predicted data is distributed normally, due to the fact that the PMF assumes a normal distribution on its ratings, and the variance of each ratings are the same. Therefore, there seems to be space for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputated_ratings = np.load('../data/zero_imputated_ratings.npy')\n",
    "is_observed = (imputated_ratings != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian PMF using Pyro\n",
    "# Reference https://www.cs.toronto.edu/~amnih/papers/bpmf.pdf\n",
    "# (To use Pyro, need data that has missing values imputated with the mean rating of each user)\n",
    "def model(data, k=10):\n",
    "    with pyro.plate('users', ratings_df.userId.nunique()):\n",
    "        with pyro.plate('user_k', k):\n",
    "            p_u_mu = pyro.sample('p_u_mu', dist.Normal(0, 1))\n",
    "            p_u_std = pyro.sample('p_u_std', dist.InverseGamma(1,1))\n",
    "            u = pyro.sample('u', dist.Normal(p_u_mu, p_u_std))\n",
    "    with pyro.plate('movies', ratings_df.movieId.nunique()):\n",
    "        with pyro.plate('movie_k', k):\n",
    "            p_v_mu = pyro.sample('p_v_mu', dist.Normal(0, 1))\n",
    "            p_v_std = pyro.sample('p_v_std', dist.InverseGamma(1,1))\n",
    "            v = pyro.sample('v', dist.Normal(p_v_mu, p_v_std))\n",
    "    with poutine.mask(mask=torch.tensor(is_observed)):\n",
    "            pyro.sample(\"obs\", dist.Normal(torch.mm(u.T, v),1), obs=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 200/200 [7:57:53, 143.37s/it, step size=1.60e-02, acc. prob=0.774]\n"
     ]
    }
   ],
   "source": [
    "pyro.set_rng_seed(2)\n",
    "kernel = NUTS(model, step_size=1e-4) \n",
    "mcmc = MCMC(kernel, num_samples=100, warmup_steps=100)\n",
    "mcmc.run(torch.tensor(imputated_ratings))\n",
    "posterior_samples = mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bpmf_posterior_samples.pickle', 'wb') as handle:\n",
    "    pickle.dump(posterior_samples, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_estimate_ratings = util_pmf.calc_average_ratings(posterior_samples['u'], posterior_samples['v'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/bpmf_rounded_posterior_samples.pickle', 'wb') as handle:\n",
    "    pickle.dump(rounded_estimate_ratings, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "util_pmf.plot_rounded_estimates(rounded_estimate_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
